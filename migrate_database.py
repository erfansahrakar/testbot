"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Migration Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
âœ… Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Indexes
âœ… Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
âœ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ

Ø§Ø³ØªÙØ§Ø¯Ù‡:
    python migrate_database.py
"""
import sqlite3
import logging
import sys
from datetime import datetime
from config import DATABASE_NAME

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_backup():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ú©Ø§Ù¾ Ù‚Ø¨Ù„ Ø§Ø² migration"""
    import shutil
    
    backup_name = f"{DATABASE_NAME}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    try:
        shutil.copy2(DATABASE_NAME, backup_name)
        logger.info(f"âœ… Ø¨Ú©Ø§Ù¾ Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {backup_name}")
        return True
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ú©Ø§Ù¾: {e}")
        return False


def add_indexes(cursor):
    """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Indexes Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Performance"""
    logger.info("ğŸ“Š Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Indexes...")
    
    indexes = [
        ("idx_orders_user_id", "CREATE INDEX IF NOT EXISTS idx_orders_user_id ON orders(user_id)"),
        ("idx_orders_status", "CREATE INDEX IF NOT EXISTS idx_orders_status ON orders(status)"),
        ("idx_orders_created_at", "CREATE INDEX IF NOT EXISTS idx_orders_created_at ON orders(created_at DESC)"),
        ("idx_cart_user_id", "CREATE INDEX IF NOT EXISTS idx_cart_user_id ON cart(user_id)"),
        ("idx_discount_code", "CREATE INDEX IF NOT EXISTS idx_discount_code ON discount_codes(code)"),
        ("idx_products_channel_msg", "CREATE INDEX IF NOT EXISTS idx_products_channel_msg ON products(channel_message_id)"),
        ("idx_packs_product_id", "CREATE INDEX IF NOT EXISTS idx_packs_product_id ON packs(product_id)"),
        ("idx_orders_status_created", "CREATE INDEX IF NOT EXISTS idx_orders_status_created ON orders(status, created_at DESC)"),
    ]
    
    created_count = 0
    for name, sql in indexes:
        try:
            cursor.execute(sql)
            logger.info(f"  âœ… {name}")
            created_count += 1
        except sqlite3.Error as e:
            logger.warning(f"  âš ï¸ {name}: {e}")
    
    logger.info(f"âœ… {created_count}/{len(indexes)} Index Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")


def add_missing_columns(cursor):
    """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯"""
    logger.info("ğŸ“‹ Ø¨Ø±Ø±Ø³ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯...")
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† per_user_limit Ø¯Ø± discount_codes
    cursor.execute("PRAGMA table_info(discount_codes)")
    columns = [col[1] for col in cursor.fetchall()]
    
    if 'per_user_limit' not in columns:
        logger.info("  ğŸ”„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† per_user_limit...")
        cursor.execute("ALTER TABLE discount_codes ADD COLUMN per_user_limit INTEGER")
        logger.info("  âœ… per_user_limit Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    else:
        logger.info("  â„¹ï¸ per_user_limit Ù‚Ø¨Ù„Ø§Ù‹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯")
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ† expires_at Ø¯Ø± orders
    cursor.execute("PRAGMA table_info(orders)")
    columns = [col[1] for col in cursor.fetchall()]
    
    if 'expires_at' not in columns:
        logger.info("  ğŸ”„ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† expires_at...")
        cursor.execute("ALTER TABLE orders ADD COLUMN expires_at TIMESTAMP")
        logger.info("  âœ… expires_at Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    else:
        logger.info("  â„¹ï¸ expires_at Ù‚Ø¨Ù„Ø§Ù‹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯")


def cleanup_old_data(cursor, days_old=30):
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
    logger.info(f"ğŸ§¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± Ø§Ø² {days_old} Ø±ÙˆØ²...")
    
    try:
        # Ø­Ø°Ù Ø³ÙØ§Ø±Ø´Ø§Øª Ø±Ø¯ Ø´Ø¯Ù‡ Ù‚Ø¯ÛŒÙ…ÛŒ
        cursor.execute("""
            DELETE FROM orders 
            WHERE status = 'rejected' 
            AND datetime(created_at) < datetime('now', '-' || ? || ' days')
        """, (days_old,))
        
        rejected_count = cursor.rowcount
        
        # Ø­Ø°Ù Ø³ÙØ§Ø±Ø´Ø§Øª Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡
        cursor.execute("""
            DELETE FROM orders 
            WHERE status = 'expired' 
            AND datetime(created_at) < datetime('now', '-' || ? || ' days')
        """, (days_old,))
        
        expired_count = cursor.rowcount
        
        logger.info(f"  âœ… {rejected_count} Ø³ÙØ§Ø±Ø´ Ø±Ø¯ Ø´Ø¯Ù‡ Ø­Ø°Ù Ø´Ø¯")
        logger.info(f"  âœ… {expired_count} Ø³ÙØ§Ø±Ø´ Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡ Ø­Ø°Ù Ø´Ø¯")
        
        return rejected_count + expired_count
    
    except Exception as e:
        logger.error(f"  âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ: {e}")
        return 0


def get_database_stats(cursor):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    logger.info("ğŸ“Š Ø¢Ù…Ø§Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³:")
    
    stats = {}
    
    # ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„Ø§Øª
    cursor.execute("SELECT COUNT(*) FROM products")
    stats['products'] = cursor.fetchone()[0]
    logger.info(f"  â€¢ Ù…Ø­ØµÙˆÙ„Ø§Øª: {stats['products']:,}")
    
    # ØªØ¹Ø¯Ø§Ø¯ Ù¾Ú©â€ŒÙ‡Ø§
    cursor.execute("SELECT COUNT(*) FROM packs")
    stats['packs'] = cursor.fetchone()[0]
    logger.info(f"  â€¢ Ù¾Ú©â€ŒÙ‡Ø§: {stats['packs']:,}")
    
    # ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
    cursor.execute("SELECT COUNT(*) FROM users")
    stats['users'] = cursor.fetchone()[0]
    logger.info(f"  â€¢ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {stats['users']:,}")
    
    # ØªØ¹Ø¯Ø§Ø¯ Ø³ÙØ§Ø±Ø´Ø§Øª
    cursor.execute("SELECT COUNT(*) FROM orders")
    stats['orders'] = cursor.fetchone()[0]
    logger.info(f"  â€¢ Ø³ÙØ§Ø±Ø´Ø§Øª: {stats['orders']:,}")
    
    # Ø³ÙØ§Ø±Ø´Ø§Øª Ø¯Ø± Ø§Ù†ØªØ¸Ø§Ø±
    cursor.execute("SELECT COUNT(*) FROM orders WHERE status = 'pending'")
    stats['pending_orders'] = cursor.fetchone()[0]
    logger.info(f"  â€¢ Ø³ÙØ§Ø±Ø´Ø§Øª Ø¯Ø± Ø§Ù†ØªØ¸Ø§Ø±: {stats['pending_orders']:,}")
    
    # ØªØ¹Ø¯Ø§Ø¯ Ú©Ø¯Ù‡Ø§ÛŒ ØªØ®ÙÛŒÙ
    cursor.execute("SELECT COUNT(*) FROM discount_codes")
    stats['discounts'] = cursor.fetchone()[0]
    logger.info(f"  â€¢ Ú©Ø¯Ù‡Ø§ÛŒ ØªØ®ÙÛŒÙ: {stats['discounts']:,}")
    
    # Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
    cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
    size_bytes = cursor.fetchone()[0]
    size_mb = size_bytes / (1024 * 1024)
    logger.info(f"  â€¢ Ø­Ø¬Ù… Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {size_mb:.2f} MB")
    
    return stats


def optimize_database(conn):
    """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    logger.info("âš¡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³...")
    
    try:
        # VACUUM Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ ÙØ¶Ø§
        conn.execute("VACUUM")
        logger.info("  âœ… VACUUM Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        
        # ANALYZE Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Query Planner
        conn.execute("ANALYZE")
        logger.info("  âœ… ANALYZE Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        
        return True
    except Exception as e:
        logger.error(f"  âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
        return False


def migrate_database():
    """Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Migration"""
    logger.info("="*60)
    logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ Migration Ø¯ÛŒØªØ§Ø¨ÛŒØ³")
    logger.info("="*60)
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¨Ú©Ø§Ù¾
    if not create_backup():
        response = input("âš ï¸ Ø¨Ú©Ø§Ù¾ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø´Ø¯! Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯ØŸ (yes/no): ")
        if response.lower() != 'yes':
            logger.info("âŒ Migration Ù„ØºÙˆ Ø´Ø¯")
            return False
    
    conn = None
    try:
        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        logger.info(f"ğŸ”Œ Ø§ØªØµØ§Ù„ Ø¨Ù‡ {DATABASE_NAME}...")
        conn = sqlite3.connect(DATABASE_NAME)
        cursor = conn.cursor()
        
        # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Foreign Keys
        cursor.execute("PRAGMA foreign_keys = ON")
        
        # Ø´Ø±ÙˆØ¹ Transaction
        cursor.execute("BEGIN")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Indexes
        add_indexes(cursor)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
        add_missing_columns(cursor)
        
        # Commit ØªØºÛŒÛŒØ±Ø§Øª
        conn.commit()
        logger.info("âœ… ØªØºÛŒÛŒØ±Ø§Øª commit Ø´Ø¯")
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        response = input("\nğŸ§¹ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒØŸ (yes/no): ")
        if response.lower() == 'yes':
            cursor.execute("BEGIN")
            deleted = cleanup_old_data(cursor, days_old=30)
            conn.commit()
            logger.info(f"âœ… {deleted} Ø±Ú©ÙˆØ±Ø¯ Ù‚Ø¯ÛŒÙ…ÛŒ Ø­Ø°Ù Ø´Ø¯")
        
        # Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
        response = input("\nâš¡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ØŸ (yes/no): ")
        if response.lower() == 'yes':
            optimize_database(conn)
        
        # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
        print("\n" + "="*60)
        get_database_stats(cursor)
        print("="*60)
        
        logger.info("\nâœ… Migration Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!")
        return True
    
    except Exception as e:
        logger.error(f"\nâŒ Ø®Ø·Ø§ Ø¯Ø± Migration: {e}")
        if conn:
            conn.rollback()
            logger.info("â†©ï¸ ØªØºÛŒÛŒØ±Ø§Øª Rollback Ø´Ø¯")
        return False
    
    finally:
        if conn:
            conn.close()
            logger.info("ğŸ”Œ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø³ØªÙ‡ Ø´Ø¯")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ“¦ Database Migration Script")
    print("="*60)
    print(f"Database: {DATABASE_NAME}")
    print("="*60 + "\n")
    
    response = input("âš ï¸ Ø¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ (yes/no): ")
    
    if response.lower() == 'yes':
        success = migrate_database()
        sys.exit(0 if success else 1)
    else:
        logger.info("âŒ Migration Ù„ØºÙˆ Ø´Ø¯")
        sys.exit(0)
